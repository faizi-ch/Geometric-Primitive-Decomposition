{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *\n",
    "from datasets import *\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Grayscale,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    CenterCrop\n",
    ")\n",
    "\n",
    "from model import PrimitivesNet\n",
    "from losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for reproducibility\n",
    "set_random_seed(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TableChair Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'datasets/table_chair.h5'\n",
    "batch_size=128\n",
    "transforms = Compose([Resize(64), Grayscale(), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(\n",
    "        data_path: str, training: bool, split_type: Literal[\"train\", \"valid\", \"test\"], batch_size: int = 32, transforms: t.Optional[\n",
    "        t.Callable[[np.ndarray], t.Union[torch.Tensor, np.ndarray]]\n",
    "    ] = None,\n",
    "    ) -> DataLoader:\n",
    "        transforms = transforms\n",
    "        loader = DataLoader(\n",
    "            dataset=TableChairDataset(data_path, split_type, transforms),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=training,\n",
    "            drop_last=training,\n",
    "            num_workers=0,\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "train_loader = dataloader(data_path, True, \"train\", batch_size=batch_size, transforms=transforms)\n",
    "valid_loader = dataloader(data_path, False, \"valid\", batch_size=batch_size, transforms=transforms)\n",
    "test_loader = dataloader(data_path, False, \"test\", batch_size=batch_size, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'datasets/simple_dataset'\n",
    "images_path = list(map(lambda x: os.path.join(os.path.abspath(data_path), x),os.listdir(data_path)))\n",
    "batch_size=128\n",
    "transforms = Compose([Resize(64), Grayscale(), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dataset = SimpleDataset(image_paths=images_path, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting dataset \n",
    "train_sampler, valid_sampler = split_dataset(simple_dataset, valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    simple_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    simple_dataset, batch_size=batch_size, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'datasets/'\n",
    "batch_size=128\n",
    "transforms = Compose([Resize(64), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "MNIST_data = MNIST(root=data_path, download=True, train=True, transform=transforms)\n",
    "MNIST_data_test = MNIST(root=data_path, download=True, train=False, transform=transforms)\n",
    "#len(MNIST_data)\n",
    "mnist_dataset = MNISTDataset(dataset=MNIST_data, transforms=transforms)\n",
    "mnist_dataset_test = MNISTDataset(dataset=MNIST_data_test, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting dataset \n",
    "train_sampler, valid_sampler = split_dataset(mnist_dataset, valid_size=0.2)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    mnist_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    mnist_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    mnist_dataset_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'datasets/pet'\n",
    "batch_size=128\n",
    "transforms = Compose([ToTensor()])\n",
    "\n",
    "tf_rgb = Compose([\n",
    "    ToTensor(),\n",
    "    Resize((64, 64))\n",
    "    #CenterCrop((64, 64))\n",
    "    #T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    \n",
    "])\n",
    "\n",
    "tf_gray = Compose([\n",
    "    #CenterCrop((64,64)),\n",
    "    Resize((64, 64)),\n",
    "    PILToTensor_for_targets(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import OxfordIIITPet\n",
    "\n",
    "pet_dataset_trainval = OxfordIIITPet(root = data_path, split=\"trainval\", target_types=\"segmentation\", transform=tf_rgb, target_transform=tf_gray, download=True)\n",
    "\n",
    "pet_dataset_test = OxfordIIITPet(root = data_path, split=\"test\", target_types=\"segmentation\", transform=tf_rgb, target_transform=tf_gray, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_dataset_trainval = PetDataset(dataset=pet_dataset_trainval, transforms=transforms)\n",
    "pet_dataset_test = PetDataset(dataset=pet_dataset_test, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting dataset \n",
    "train_sampler, valid_sampler = split_dataset(pet_dataset_trainval, valid_size=0.2)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    pet_dataset_trainval, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    pet_dataset_trainval, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    pet_dataset_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data and display it\n",
    "inputs = next(iter(train_loader))\n",
    "\n",
    "show_grid(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 1 # 3 for RGB images\n",
    "latent_size = 256 # final output size of encoder\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 50\n",
    "num_shape_type = 8 # number of shapes per type\n",
    "threshold = 0.5 #Thresholding value for weights. If weight > threshold, 1 else 0\n",
    "\n",
    "prev_CD = 100\n",
    "\n",
    "model_name = 'primitives_net_ct13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PrimitivesNet(num_channels=num_channels, latent_size=latent_size)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard_logger\n",
    "from tensorboard_logger import log_value\n",
    "\n",
    "tensorboard_logger.configure(\"logs/tensorboard/{}\".format(model_name), flush_secs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    net.train()\n",
    "    pbar = tqdm(total=len(train_loader.dataset), leave=False)\n",
    "    epoch_str = '' if epoch is None else '[Epoch {}/{}]'.format(\n",
    "            str(epoch).zfill(len(str(num_epochs))), num_epochs)\n",
    "\n",
    "    train_loss = 0.0\n",
    "    n = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        image, pt, dist = batch\n",
    "        \n",
    "        image = image.to(device)\n",
    "        pt = pt.to(device)\n",
    "        dist = dist.to(device)\n",
    "\n",
    "        pred = net(image, pt)\n",
    "\n",
    "        loss = total_loss(pred, dist, net.scaler, net.shape_evaluator, net.boolean_layer)\n",
    "     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        n += 1\n",
    " \n",
    "        pbar.set_description('{} {} Loss: {:f}'.format(epoch_str, 'Train', loss.item()))\n",
    "        pbar.update(image.shape[0])\n",
    "\n",
    "    pbar.close()\n",
    "    mean_train_loss = train_loss / n\n",
    "\n",
    "    print(\"Epoch {}/{} => train_loss: {}\".format(epoch, num_epochs, mean_train_loss))\n",
    "    log_value('train_loss', mean_train_loss, epoch)\n",
    "\n",
    "    # evaluation\n",
    "    net.eval()\n",
    "    pbar = tqdm(total=len(valid_loader.dataset), leave=False)\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    valid_CD = 0.0\n",
    "    valid_IoU = 0.0\n",
    "    n = 0.0\n",
    "\n",
    "    for batch in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            image, pt, dist = batch\n",
    "            image = image.to(device)\n",
    "            pt = pt.to(device)\n",
    "            dist = dist.to(device)\n",
    "            pred = net(image, pt)\n",
    "\n",
    "            loss = total_loss(pred, dist, net.scaler, net.shape_evaluator, net.boolean_layer)\n",
    "            valid_loss += loss.item()\n",
    "            n += 1\n",
    "\n",
    "            #pred = net.binarize(pred).clone()#.reshape(-1, 64, 64).clone().cpu().numpy()\n",
    "            pred = net.binarize(pred).reshape(-1, 64, 64).clone().cpu().numpy()\n",
    "            dist = dist.reshape(-1, 64, 64).clone().cpu().numpy()\n",
    "\n",
    "            CD = chamfer_distance(pred, dist)\n",
    "            IoU = iou(pred, dist)\n",
    "            valid_CD += CD\n",
    "            valid_IoU += IoU\n",
    "\n",
    "        pbar.set_description('{} {} Loss: {:f}, CD: {:f}, IoU: {:f}%'.format(epoch_str, 'Valid', loss.item(), CD, IoU*100))\n",
    "        pbar.update(image.shape[0])\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    mean_valid_loss = valid_loss / n\n",
    "    valid_CD = valid_CD / n \n",
    "    valid_IoU = valid_IoU / n\n",
    "    log_value('valid_loss', mean_valid_loss, epoch)\n",
    "    log_value('chamfer_distance', valid_CD, epoch)\n",
    "    log_value('IoU', valid_IoU, epoch)\n",
    "\n",
    "    print(\"Epoch {}/{} => valid_loss: {:f}, CD: {:f}, IoU: {:f}%\".format(epoch, num_epochs, mean_valid_loss, valid_CD, valid_IoU*100))\n",
    "\n",
    "    # save model\n",
    "    if prev_CD > valid_CD:\n",
    "        print(\"Saving the Model based on Chamfer Distance: %f\"%(valid_CD), flush=True)\n",
    "        torch.save(net.state_dict(), \"trained_models/{}.pth\".format(model_name))\n",
    "        prev_CD = valid_CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "net.load_state_dict(torch.load(\"trained_models/{}.pth\".format(model_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cad dataset\n",
    "\n",
    "boxes = []\n",
    "circles = []\n",
    "coverage_cir = []\n",
    "triangles = []\n",
    "coverage_tri = []\n",
    "coverage_rect = []\n",
    "\n",
    "for batch in valid_loader:\n",
    "    #optimizer.zero_grad()\n",
    "    \n",
    "    #image, pt, dist, bounding_volume = batch\n",
    "    image, pt, dist = batch\n",
    "\n",
    "    image = image.to(device)\n",
    "    pt = pt.to(device)\n",
    "    dist = dist.to(device)\n",
    "\n",
    "    recon, pred = net(image, pt, return_shapes_distances=True)\n",
    "\n",
    "    pred = pred.permute((0, 2, 1)) #[batch, num_pt, num_shape]\n",
    "    #print(len(pred))\n",
    "    # generate random number between 0 and 128\n",
    "    rand_num = random.randint(0, batch_size-1)\n",
    "\n",
    "    gt_img = image[rand_num].detach().cpu()\n",
    "    img = image[rand_num].cpu().detach().numpy()\n",
    "    img = img.transpose((1, 2, 0))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    #pred = net.binarize(pred).reshape(-1, 64, 64)#.clone().cpu().numpy()\n",
    "    #pred = pred.clamp(0, 1)\n",
    "    pred = net.binarize(pred)\n",
    "\n",
    "    #gt_dist = dist[rand_num].clone()\n",
    "    #dist = dist.reshape(-1, 64, 64).clone().cpu().numpy()\n",
    "\n",
    "    #plt.imshow(dist[0])\n",
    "    #plt.show()\n",
    "\n",
    "    recon = recon[rand_num].clone().detach().cpu().numpy()\n",
    "    #print(\"recon shape: \",recon.shape)\n",
    "    print(\"reconstructed:\")\n",
    "    plt.imshow(recon.reshape(64, 64), cmap=\"viridis\")\n",
    "    #pred_dist = pred_dist.reshape(-1, 64, 64)\n",
    "    #plt.imshow(recon[0])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8*2, 4*2))\n",
    "\n",
    "    for i, prd in enumerate(pred[rand_num]):\n",
    "        #print(prd.shape)\n",
    "\n",
    "        #for dis in prd:\n",
    "        p = prd.cpu().detach().numpy().reshape(64, 64)\n",
    "\n",
    "        \n",
    "\n",
    "        prd_m = torch.unsqueeze(prd.reshape(64, 64), 0)\n",
    "        #print(prd_m.shape)\n",
    "\n",
    "        coverage = coverage_threshold(gt_img.cpu().detach(), prd_m.cpu().detach())\n",
    "        #print(\"coverage: \",coverage)\n",
    "        coverage = coverage.item() if isinstance(coverage, torch.Tensor) else coverage\n",
    "\n",
    "        plt.subplot(4, 8, i+1)\n",
    "        plt.imshow(p, cmap=\"viridis\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(round(coverage, 2))\n",
    "\n",
    "        # if coverage < 0.4:\n",
    "        #     continue\n",
    "\n",
    "        contours, hierarchy = cv2.findContours((255*prd.cpu().detach().numpy().reshape(64, 64).copy()).astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        #_,contours,_ = cv2.findContours(prd.cpu().detach().numpy().reshape(64, 64, 1).copy(), 1, 1)\n",
    "        \n",
    "        # if i <= 7:\n",
    "        #     cir = cv2.minEnclosingCircle(contours[0])\n",
    "\n",
    "        #     (x,y), radius = cir\n",
    "        #     center = (int(x), int(y))\n",
    "        #     radius = int(radius)\n",
    "        #     circles.append((center, radius))\n",
    "        # else:\n",
    "        #     if coverage.item() < 0.9:\n",
    "        #         continue\n",
    "\n",
    "        if i <= 7:\n",
    "            \n",
    "            rect = cv2.minAreaRect(contours[0])\n",
    "\n",
    "            (x,y),(w,h), a = rect\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.int0(box) #turn into ints\n",
    "            boxes.append(box)\n",
    "\n",
    "            coverage_rect.append(coverage)\n",
    "\n",
    "        else:\n",
    "\n",
    "            \n",
    "            cir = cv2.minEnclosingCircle(contours[0])\n",
    "\n",
    "            (x,y), radius = cir\n",
    "            center = (int(x), int(y))\n",
    "            radius = int(radius)\n",
    "            circles.append((center, radius))\n",
    "            coverage_cir.append(coverage)\n",
    "            \n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    blank_img = np.ones((64, 64, 3)) * 255 #img.copy()#np.zeros((64, 64, 1))\n",
    "    blank_img[:,:,0] = recon.reshape(64, 64)\n",
    "    blank_img[:,:,1] = recon.reshape(64, 64)\n",
    "    blank_img[:,:,2] = recon.reshape(64, 64)\n",
    "\n",
    "    for bbox in boxes:\n",
    "        cv2.drawContours(blank_img, [bbox], 0, (1, 0, 0), 1)\n",
    "\n",
    "    for i, (cent, rad) in enumerate(circles):\n",
    "        cv2.circle(blank_img, cent, rad, (0,1,0), 1)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "    outs = [img, recon.reshape(64, 64), blank_img]\n",
    "    lbls = [\"Input\", \"Reconstructed\", \"Primitives\"]\n",
    "    for i in range(3):\n",
    "        plt.subplot(1,3,i+1) \n",
    "        plt.imshow(outs[i])\n",
    "        #plt.xlabel(lbls[i])\n",
    "        plt.title(lbls[i])\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "385c13088ed99ad8e3b12d76a9483f55f3954ec0bf6653e756cd2bbb35911f22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
